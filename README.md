# SGD-vs-GD-Implementation-Simulation
  
## üìù Description
-The increase of the size of the databases that need to be processed daily leads to the increase of the interest put on large scale optimization problems, which increase remarkably. This, in fact, represents a fundamental issue for machine learning algorithms. In this case, the tasks requisite solving an optimization problem of a composite loss function, whereby the number of terms scales with the number of observations. Alternative learning strategies that can handle large databases need to be developed, since the traditional techniques grounded in full gradient information at each update step are no longer time-sufficient. It is, thus, crucial to do away with traditional plain methods used to build strategies that keep a light ‚Äôper update iteration‚Äô cost.This is the main focus of stochastic gradient methods which is not a new concept (cf. ADALINE, 1960), but is of crucial importance today, mainly for training machine learning and resolving minimization problems in finance.

## ‚è≥ Performed Tasks :
-  A literature Analysis and presentation about Gradient Descent and Stochastic Gradient Descent.
- Using Python to implement GD and SGD functions
- Using the implemeted functions for minimizing the mean squared error on the OCR dataset
- Implementing the comparison function between GD and SGD Based on the OCR dataset and MSE
- Interpreting from the given results and plots the Pros and cons of every algorithms
- Stochastic Processes Simulation
- -  Brownian Motion or Wiener Process Simulation
- -  Poisson Process Simulation
